{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from FilesFunc import files\n",
    "folders = [\"Background\", \"BH\", \"Sphaleron\"]\n",
    "stuffs = [\"electron\", \"jet\", \"MET\", \"muon\", \"photon\", \"tau\"]\n",
    "data_variables = [\"HT\", \"met\", \"phi_diff\", \"ptmax\", \"stuff_amount\"]\n",
    "file_amounts = [0, 18, 3]\n",
    "font = {'family': 'Times New Roman',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal'\n",
    "        }\n",
    "\n",
    "\n",
    "filenames_dict = {\n",
    "    \"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL\": \"Sphal1\",\n",
    "    \"PP13-Sphaleron-THR9-FRZ15-NB33-60-NSUBP50\": \"Sphal2\",\n",
    "    \"PP13-Sphaleron-THR9-FRZ15-NB33-71-NSUBP5\": \"Sphal3\",\n",
    "    \"BH_n4_M8\": \"BH1\",\n",
    "    \"BH_n4_M9\": \"BH2\",\n",
    "    \"BH_n4_M9_Mpl9\": \"BH3\",\n",
    "    \"BH_n4_M10\": \"BH4\",\n",
    "    \"BH_n4_M11\": \"BH5\",\n",
    "    \"BH_n4_M12\": \"BH6\",\n",
    "    \"BH_n5_M8\": \"BH7\",\n",
    "    \"BH_n5_M9\": \"BH8\",\n",
    "    \"BH_n5_M10\": \"BH9\",\n",
    "    \"BH_n5_M11\": \"BH10\",\n",
    "    \"BH_n5_M12\": \"BH11\",\n",
    "    \"BH_n6_M8\": \"BH12\",\n",
    "    \"BH_n6_M9\": \"BH13\",\n",
    "    \"BH_n6_M10\": \"BH14\",\n",
    "    \"BH_n6_M11\": \"BH15\",\n",
    "    \"BH_n6_M12\": \"BH16\",\n",
    "    \"BlackMaxOutputFirstRun1\": \"BH17\",\n",
    "    \"BlackMaxOutputFirstRun2\": \"BH18\",\n",
    "    \"ttbar_largejet\": \"ttbar1\",\n",
    "    \"ttbar\": \"ttbar2\"\n",
    "}\n",
    "\n",
    "\n",
    "def work_space(path):\n",
    "    while True:\n",
    "        if os.path.split(path)[1] != \"Phys117\":\n",
    "            path = os.path.split(path)[0]\n",
    "        else:\n",
    "            return path.replace(\"\\\\\", \"/\")\n",
    "work_dir = work_space(os.getcwd())\n",
    "\n",
    "\n",
    "def remover(old_list, index):\n",
    "    new_list = [element for element_index, element in enumerate(old_list) if element_index != index]\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == list or type(nested_list) == tuple:\n",
    "            list_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def list_unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == list:\n",
    "            list_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def tuple_unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == tuple:\n",
    "            tuple_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def eff_retriever(data_path, combine_data):\n",
    "    filenames = os.listdir(data_path)\n",
    "    filepaths = [data_path + filename for filename in filenames]\n",
    "    if combine_data:\n",
    "        dataframes = [pd.read_csv(filepath) for filepath in filepaths if filepath[-len(\"Combined.csv\"):] == \"Combined.csv\"]\n",
    "    else:\n",
    "        dataframes = [pd.read_csv(filepath) for filepath in filepaths if filepath[-len(\"Combined.csv\"):] != \"Combined.csv\"]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def dropper(dataframe, data_variables):\n",
    "    to_drop = [col for col in dataframe.columns if col not in data_variables]\n",
    "    dataframe = dataframe.drop(to_drop, axis = 1)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def dataframe_retriever(data_path, data_variables):\n",
    "    from natsort import natsorted, ns\n",
    "    folders = os.listdir(data_path)\n",
    "    folder_filenames = [natsorted([filename for filename in os.listdir(data_path + folder)], key = lambda y: y.lower()) for folder in folders]\n",
    "    folder_filepaths = [[data_path + folder + \"/\" + filename for filename in folder_filenames[folder_index]] for folder_index, folder in enumerate(folders)]\n",
    "    folder_filenames = [[filenames_dict[filename[:-len(\".csv\")]] for filename in filenames] for filenames in folder_filenames]\n",
    "    dataframes = [[(pd.read_csv(filepath).drop(\"Unnamed: 0\", axis = 1), filename) for filepath, filename in zip(filepaths, filenames)] for filepaths, filenames in zip(folder_filepaths, folder_filenames)]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def selector(folder_dataframes, folder_filenames):\n",
    "    dataframes = [[dataframe[0] for dataframe in dataframes if dataframe[1] in filenames] for dataframes, filenames in zip(folder_dataframes, folder_filenames)]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def eff_selector(combine_data, dataframes):\n",
    "    if combine_data:\n",
    "        dataframes = [dataframe[0].drop(\"Unnamed: 0\", axis = 1).set_index(dataframe[0][\"Unnamed: 0\"]) for dataframe in dataframes if dataframe[1][-len(\"Combined.csv\"):] == \"Combined.csv\"]\n",
    "    else:\n",
    "        dataframes = [dataframe[0].drop(\"Unnamed: 0\", axis = 1).set_index(dataframe[0][\"Unnamed: 0\"]) for dataframe in dataframes if dataframe[1][-len(\"Combined.csv\"):] != \"Combined.csv\"]\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def sampler(model_dataframes, file_amounts, data_variables):\n",
    "    from random import sample\n",
    "    dataframe_tuples = []\n",
    "    for file_amount, dataframes in zip(file_amounts, model_dataframes):\n",
    "        dataframe_tuples.append(sample(dataframes, file_amount)) if type(file_amount) == int else dataframe_tuples.append([dataframe for dataframe in dataframes if dataframe[1] in file_amount])\n",
    "    dataframe_tuples = [sorted(dataframes, key = lambda x: int(x[1][len(string):])) for dataframes, string in zip(dataframe_tuples, [\"ttbar\", \"BH\", \"Sphal\"])]\n",
    "    \n",
    "    foldered_filenames = [[dataframe[1] for dataframe in dataframes] for dataframes in dataframe_tuples]\n",
    "    foldered_dataframes = [[dropper(dataframe[0], data_variables) for dataframe in dataframes] for dataframes in dataframe_tuples]\n",
    "\n",
    "    return foldered_dataframes, foldered_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def xy_data(dataframes_input):\n",
    "    sample_num = np.min([len(dataframe.iloc[:]) for dataframe in unpacker(dataframes_input, [])])\n",
    "    sampled_model_dataframes = [dataframe.sample(n = sample_num) for dataframe in unpacker(dataframes_input, [])]\n",
    "\n",
    "    datasets_dict = {\"datasets\": [dataset_index for dataset_index, dataframes in enumerate(sampled_model_dataframes) for i in range(sample_num)]}\n",
    "    datasets = pd.DataFrame(datasets_dict)\n",
    "\n",
    "    model_data = pd.concat(unpacker(sampled_model_dataframes, []))\n",
    "\n",
    "    return model_data, datasets\n",
    "\n",
    "\n",
    "def splitter(combine, dataframes_input):\n",
    "    splits = []\n",
    "    dataset_index = 0\n",
    "    for condition, dataframes in zip(combine, dataframes_input):\n",
    "        if condition:\n",
    "            start_index = dataset_index\n",
    "            stop_index = start_index + len(dataframes)\n",
    "            split = [i for i in range(start_index, stop_index)]\n",
    "            splits.append(split)\n",
    "            dataset_index += len(dataframes)\n",
    "        else:\n",
    "            for dataframe in dataframes:\n",
    "                splits.append([dataset_index])\n",
    "                dataset_index += 1\n",
    "        \n",
    "    return splits\n",
    "\n",
    "\n",
    "def model(dataframes_input, filenames_input, folders, combine):\n",
    "    xlabels = list_unpacker([folders[folder_index] if condition else filenames for folder_index, (filenames, condition) in enumerate(zip(filenames_input, combine))], [])\n",
    "\n",
    "    splits = splitter(combine, dataframes_input)\n",
    "\n",
    "    x_train, y_train = xy_data(dataframes_input)\n",
    "    \n",
    "    \n",
    "    if len(xlabels) != 2:\n",
    "        # Define the model and train it\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"multi:softprob\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss')\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"multi:softprob\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss', max_depth = 4)\n",
    "\n",
    "    else:\n",
    "        # Bruk disse om det er 2 forskjellige type dataset, derfor \"binary\"\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss')\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss', max_depth = 4)\n",
    "\n",
    "\n",
    "    # Train model on data\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    return xgb_model, xlabels, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = work_dir + \"/Markus/Hub/VariableData/\"\n",
    "folder_dataframes = dataframe_retriever(data_path, data_variables)\n",
    "\n",
    "combine_data = True\n",
    "combine = [True, True, True]\n",
    "data_path = work_dir + \"/Markus/Hub/Efficiencies/EfficiencyData/\"\n",
    "eff_dataframes_input = eff_retriever(data_path, combine_data)\n",
    "file_amounts = [2, [\"BH6\", \"BH11\", \"BH16\"], [\"Sphal1\"]]\n",
    "\n",
    "if not combine_data:\n",
    "    try:\n",
    "        remove_index = folders.index(\"Background\")\n",
    "        folders = remover(folders, remove_index)\n",
    "        folder_dataframes = remover(folder_dataframes, remove_index)\n",
    "        combine = remover(combine, remove_index)\n",
    "        file_amounts = remover(file_amounts, remove_index)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_variables = [\"HT\", \"met\", \"phi_diff\", \"ptmax\"]\n",
    "\n",
    "model_dataframes, model_filenames = sampler(folder_dataframes, file_amounts, data_variables)\n",
    "xgb_model, xlabels, splits = model(model_dataframes, model_filenames, folders, combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plotter() got an unexpected keyword argument 'figsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPlotter\u001b[39;00m \u001b[39mimport\u001b[39;00m plotter\n\u001b[0;32m     27\u001b[0m     plot_dataframes, plot_filenames \u001b[39m=\u001b[39m sampler(folder_dataframes, file_amounts, data_variables)\n\u001b[1;32m---> 28\u001b[0m     plotter(data_variables, plot_dataframes, plot_filenames, colors, filter_strengths \u001b[39m=\u001b[39;49m [\u001b[39m0.99\u001b[39;49m, \u001b[39m0.95\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m0.985\u001b[39;49m, \u001b[39m1\u001b[39;49m], binsizes \u001b[39m=\u001b[39;49m [\u001b[39m50\u001b[39;49m, \u001b[39m15\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m25\u001b[39;49m, \u001b[39m0.5\u001b[39;49m], figsize \u001b[39m=\u001b[39;49m \u001b[39m16\u001b[39;49m)\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     file_amounts \u001b[39m=\u001b[39m [\u001b[39m2\u001b[39m, \u001b[39m18\u001b[39m, \u001b[39m3\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: plotter() got an unexpected keyword argument 'figsize'"
     ]
    }
   ],
   "source": [
    "def split_data(y_pred, file_amounts):\n",
    "    filenum = np.sum([file_amount if type(file_amount) == int else len(file_amount) for file_amount in file_amounts])\n",
    "    split_num = int(len(y_pred) / filenum)\n",
    "    y_pred = [y_pred[i * split_num : (i + 1) * split_num] for i in range(filenum)]\n",
    "\n",
    "    y_pred_data = [tuple(np.round(y_pred[j].count(i) / split_num, 3) for i in range(len(folders))) for j in range(len(y_pred))]\n",
    "    y_pred_labels = [tuple(str(y_pred[j].count(i)) + \" - \" + str(np.round(y_pred[j].count(i) * 100 / split_num, 1)) + \"%\" for i in range(len(folders))) for j in range(len(y_pred))]\n",
    "\n",
    "    return y_pred_data, y_pred_labels\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    file_amounts = [2, [\"BH6\", \"BH11\", \"BH16\"], 3]\n",
    "    temp_amounts = [file_amount if type(file_amount) == int else len(file_amount) for file_amount in file_amounts]\n",
    "    colors = []\n",
    "    for index, file_amount in enumerate(temp_amounts):\n",
    "        temp_list = []\n",
    "        for amount in range(1, 1 + file_amount):\n",
    "            rgb = [0, 0, 0]\n",
    "            rgb[index] = (amount + 3) * 1 / (file_amount + 6)\n",
    "            rgb[int((index + 2) % 3)] = (amount + 3) * 1 / (file_amount + 6)\n",
    "            temp_list.append(rgb)\n",
    "        colors.append(temp_list) \n",
    "        \n",
    "    from Plotter import plotter\n",
    "    plot_dataframes, plot_filenames = sampler(folder_dataframes, file_amounts, data_variables)\n",
    "    plotter(data_variables, plot_dataframes, plot_filenames, colors, filter_strengths = [0.99, 0.95, 1, 0.985, 1], binsizes = [50, 15, 0.2, 25, 0.5], figsize = 16)\n",
    "\n",
    "\n",
    "if True:\n",
    "    file_amounts = [2, 18, 3]\n",
    "    dataframes_input, filenames_input = sampler(folder_dataframes, file_amounts, data_variables)\n",
    "\n",
    "    x_test, y_test = xy_data(dataframes_input)\n",
    "    ylabels = list_unpacker(filenames_input, [])\n",
    "\n",
    "    # Make predictions with model\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    y_pred = [split_index for pred_num in y_pred for split_index, split in enumerate(splits) if pred_num in split]\n",
    "\n",
    "\n",
    "    y_pred_data, y_pred_labels = split_data(y_pred, file_amounts)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (12, 11))\n",
    "    plt.style.use(\"seaborn-v0_8-dark\")\n",
    "    axs = fig.subplots(2, 1)\n",
    "\n",
    "\n",
    "    x_ticks = np.arange(len(xlabels))   \n",
    "    y_ticks = np.arange(len(ylabels))    \n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    ax = axs[0]\n",
    "    sns.heatmap(y_pred_data, annot = y_pred_labels, fmt = \"\", ax = ax, xticklabels = xlabels, yticklabels = ylabels)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), va = \"top\", rotation = 45)\n",
    "\n",
    "\n",
    "    # Plot variables by importance for prediction\n",
    "    ax = axs[1]\n",
    "    ax.grid()\n",
    "    sorted_idx = xgb_model.feature_importances_.argsort()\n",
    "    ax.barh(np.array(data_variables)[sorted_idx], xgb_model.feature_importances_[sorted_idx])\n",
    "    ax.set_xlabel(\"Xgboost Feature Importance\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "351a53613c0e6165dc63939a7e87ffd1b4ac15e0da0703e79b3ccd4b8da3e283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
