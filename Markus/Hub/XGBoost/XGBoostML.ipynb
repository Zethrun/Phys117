{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from FilesFunc import files\n",
    "folders = [\"Background\", \"BH\", \"Sphaleron\"]\n",
    "stuffs = [\"electron\", \"jet\", \"MET\", \"muon\", \"photon\", \"tau\"]\n",
    "data_variables = [\"HT\", \"met\", \"phi_diff\", \"ptmax\", \"stuff_amount\"]\n",
    "file_amounts = [0, 18, 3]\n",
    "font = {'family': 'Times New Roman',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal'\n",
    "        }\n",
    "\n",
    "def work_space(path):\n",
    "    while True:\n",
    "        if os.path.split(path)[1] != \"Phys117\":\n",
    "            path = os.path.split(path)[0]\n",
    "        else:\n",
    "            return path.replace(\"\\\\\", \"/\")\n",
    "work_dir = work_space(os.getcwd())\n",
    "\n",
    "\n",
    "def remover(old_list, index):\n",
    "    new_list = [element for element_index, element in enumerate(old_list) if element_index != index]\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == list or type(nested_list) == tuple:\n",
    "            list_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def list_unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == list:\n",
    "            list_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def tuple_unpacker(folder_data, new_folder_data):\n",
    "    for nested_list in folder_data:\n",
    "        if type(nested_list) == tuple:\n",
    "            tuple_unpacker(nested_list, new_folder_data)\n",
    "        else:\n",
    "            new_folder_data.append(nested_list)\n",
    "    folder_data = new_folder_data\n",
    "    return folder_data\n",
    "\n",
    "\n",
    "def eff_retriever(data_path, combine_data):\n",
    "    filenames = os.listdir(data_path)\n",
    "    filepaths = [data_path + filename for filename in filenames]\n",
    "    if combine_data:\n",
    "        dataframes = [pd.read_csv(filepath) for filepath in filepaths if filepath[-len(\"Combined.csv\"):] == \"Combined.csv\"]\n",
    "    else:\n",
    "        dataframes = [pd.read_csv(filepath) for filepath in filepaths if filepath[-len(\"Combined.csv\"):] != \"Combined.csv\"]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def dropper(dataframe, data_variables):\n",
    "    to_drop = [col for col in dataframe.columns if col not in data_variables]\n",
    "    dataframe = dataframe.drop(to_drop, axis = 1)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def dataframe_retriever(data_path, data_variables):\n",
    "    folders = os.listdir(data_path)\n",
    "    folder_filenames = [os.listdir(data_path + folder) for folder in folders]\n",
    "    folder_filepaths = [[data_path + folder + \"/\" + filename for filename in folder_filenames[folder_index]] for folder_index, folder in enumerate(folders)]\n",
    "    dataframes = [[(pd.read_csv(filepath).drop(\"Unnamed: 0\", axis = 1), filename) for filepath, filename in zip(filepaths, filenames)] for filepaths, filenames in zip(folder_filepaths, folder_filenames)]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def selector(folder_dataframes, folder_filenames):\n",
    "    dataframes = [[dataframe[0] for dataframe in dataframes if dataframe[1] in filenames] for dataframes, filenames in zip(folder_dataframes, folder_filenames)]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def eff_selector(combine_data, dataframes):\n",
    "    if combine_data:\n",
    "        dataframes = [dataframe[0].drop(\"Unnamed: 0\", axis = 1).set_index(dataframe[0][\"Unnamed: 0\"]) for dataframe in dataframes if dataframe[1][-len(\"Combined.csv\"):] == \"Combined.csv\"]\n",
    "    else:\n",
    "        dataframes = [dataframe[0].drop(\"Unnamed: 0\", axis = 1).set_index(dataframe[0][\"Unnamed: 0\"]) for dataframe in dataframes if dataframe[1][-len(\"Combined.csv\"):] != \"Combined.csv\"]\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def sampler(model_dataframes, file_amounts):\n",
    "    from random import sample\n",
    "    new_dataframes = []\n",
    "    for file_amount, dataframes in zip(file_amounts, model_dataframes):\n",
    "        new_dataframes.append(sample(dataframes, file_amount)) if type(file_amount) == int else new_dataframes.append([dataframe for dataframe in dataframes if dataframe[1] in file_amount])\n",
    "    return new_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def xy_data(dataframes_input, filenames_input):\n",
    "    sample_num = np.min([len(dataframe.iloc[:]) for dataframe in unpacker(dataframes_input, [])])\n",
    "    sampled_model_dataframes = [dataframe.sample(n = sample_num) for dataframe in unpacker(dataframes_input, [])]\n",
    "\n",
    "    datasets_dict = {\"datasets\": [dataset_index for dataset_index, dataframe in enumerate(sampled_model_dataframes) for i in range(len(dataframe))]}\n",
    "    datasets = pd.DataFrame(datasets_dict)\n",
    "\n",
    "    model_data = pd.concat(unpacker(sampled_model_dataframes, []))\n",
    "\n",
    "    return model_data, datasets\n",
    "\n",
    "\n",
    "def splitter(combine, dataframes_input):\n",
    "    splits = []\n",
    "    dataset_index = 0\n",
    "    for condition, dataframes in zip(combine, dataframes_input):\n",
    "        if condition:\n",
    "            start_index = dataset_index\n",
    "            stop_index = start_index + len(dataframes)\n",
    "            split = [i for i in range(start_index, stop_index)]\n",
    "            splits.append(split)\n",
    "            dataset_index += len(dataframes)\n",
    "        else:\n",
    "            for dataframe in dataframes:\n",
    "                splits.append([dataset_index])\n",
    "                dataset_index += 1\n",
    "        \n",
    "    return splits\n",
    "\n",
    "\n",
    "def model(dataframes_input, filenames_input, folders, combine):\n",
    "    xlabels = list_unpacker([folders[folder_index] if condition else filenames for folder_index, (filenames, condition) in enumerate(zip(filenames_input, combine))], [])\n",
    "\n",
    "    splits = splitter(combine, dataframes_input)\n",
    "\n",
    "    x_train, y_train = xy_data(dataframes_input, filenames_input)\n",
    "    \n",
    "\n",
    "    if len(xlabels) != 2:\n",
    "        # Define the model and train it\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"multi:softprob\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss')\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"multi:softprob\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss', max_depth = 4)\n",
    "\n",
    "    else:\n",
    "        # Bruk disse om det er 2 forskjellige type dataset, derfor \"binary\"\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss')\n",
    "        xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", random_state = 22, use_label_encoder = False, eval_metric = 'mlogloss', max_depth = 4)\n",
    "\n",
    "\n",
    "    # Train model on data\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    return xgb_model, xlabels, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eff_tup_retriever(eff_dataframes_input, filenames_input, data_variables, folders, combine):\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "#     eff_tups = []\n",
    "#     for variable, dataframe in zip(data_variables, eff_dataframes_input):\n",
    "#         for dataframe, filename in zip()\n",
    "#         if combine_data:\n",
    "#             eff_tup = dataframe[folders.index(\"Sphaleron\")][folders.index(\"BH\")]\n",
    "#             eff_tup = tuple(float(element) for element in eff_tup.strip(\"()\").split(\", \"))\n",
    "#             eff_tups.append(eff_tup)\n",
    "#         else:\n",
    "#             eff_tup = dataframe[sphal_file][bh_file]\n",
    "#             eff_tup = tuple(float(element) for element in eff_tup.strip(\"()\").split(\", \"))\n",
    "#             eff_tups.append(eff_tup)\n",
    "\n",
    "#     return eff_tups, data_variables\n",
    "\n",
    "\n",
    "# def EventEvaluator(dataframes, eff_tups, data_variables):\n",
    "#     bh_dataframe, sphal_dataframe = dataframes\n",
    "#     evaluations = []\n",
    "#     efficiencies = np.array([eff_tup[1] for eff_tup in eff_tups])\n",
    "#     weight = np.sum(efficiencies)\n",
    "        \n",
    "#     for event_index in range(len(bh_dataframe)):\n",
    "#         event = bh_dataframe.iloc[event_index]\n",
    "#         bh = []\n",
    "#         for eff_tup, variable in zip(eff_tups, data_variables):\n",
    "#             eff_val, eff_acc, eff_rel = eff_tup\n",
    "#             event_variable = event[variable]\n",
    "#             if eff_rel == 1:\n",
    "#                 if event_variable > eff_val:\n",
    "#                     bh.append(eff_acc)\n",
    "#                 else:\n",
    "#                     bh.append(1 - eff_acc)\n",
    "#             else:\n",
    "#                 if event_variable < eff_val:\n",
    "#                     bh.append(eff_acc)\n",
    "#                 else:\n",
    "#                     bh.append(1 - eff_acc)\n",
    "\n",
    "#         if np.sum(bh) / weight >= 0.5:\n",
    "#             evaluations.append((0, 0))\n",
    "#         else:\n",
    "#             evaluations.append((1, 0))\n",
    "\n",
    "#     for event_index in range(len(sphal_dataframe)):\n",
    "#         event = sphal_dataframe.iloc[event_index]\n",
    "#         sphal = []\n",
    "#         for eff_tup, variable in zip(eff_tups, data_variables):\n",
    "#             eff_val, eff_acc, eff_rel = eff_tup\n",
    "#             event_variable = event[variable]\n",
    "#             if eff_rel == 1:\n",
    "#                 if event_variable < eff_val:\n",
    "#                     sphal.append(eff_acc)\n",
    "#                 else:\n",
    "#                     sphal.append(1 - eff_acc)\n",
    "#             else:\n",
    "#                 if event_variable > eff_val:\n",
    "#                     sphal.append(eff_acc)\n",
    "#                 else:\n",
    "#                     sphal.append(1 - eff_acc)\n",
    "\n",
    "#         if np.sum(sphal) / weight >= 0.5:\n",
    "#             evaluations.append((1, 1))\n",
    "#         else:\n",
    "#             evaluations.append((0, 1))\n",
    "\n",
    "#     return evaluations\n",
    "\n",
    "\n",
    "# def evaluations_plot(evaluations, eff_tups):\n",
    "#     data_array = [[0, 0],\n",
    "#                   [0, 0]]\n",
    "\n",
    "#     for evaluation in evaluations:\n",
    "#         data_array[evaluation[1]][evaluation[0]] += 1\n",
    "\n",
    "#     data_array_norm = [np.round(data_array[0] / np.sum(data_array[0]), 2),\n",
    "#                        np.round(data_array[1] / np.sum(data_array[1]), 2)]\n",
    "    \n",
    "#     array, array_norm = np.array(data_array), np.array(data_array_norm)\n",
    "\n",
    "#     fig = plt.figure(figsize = (20, 6))\n",
    "#     plt.style.use(\"seaborn-v0_8-dark\")\n",
    "#     subplots = fig.subplots(1, 3)\n",
    "\n",
    "#     ax = subplots[0]\n",
    "#     conf = array\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix = conf, display_labels = folders)\n",
    "#     disp.plot(ax = ax)\n",
    "\n",
    "#     ax = subplots[1]\n",
    "#     conf = array_norm\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix = conf, display_labels = folders)\n",
    "#     disp.plot(ax = ax)\n",
    "\n",
    "#     ax = subplots[2]\n",
    "#     ax.grid()\n",
    "#     efficiencies = np.array([eff_tup[1] for eff_tup in eff_tups])\n",
    "#     efficiencies /= np.sum(efficiencies)\n",
    "#     sorted_idx = efficiencies.argsort()\n",
    "#     ax.barh(np.array(data_variables)[sorted_idx], efficiencies[sorted_idx])\n",
    "#     ax.set_xlabel(\"Xgboost Feature Importance\")\n",
    "\n",
    "\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = work_dir + \"/Markus/Hub/VariableData/\"\n",
    "model_dataframes = dataframe_retriever(data_path, data_variables)\n",
    "\n",
    "combine_data = False\n",
    "# data_path = work_dir + \"/Markus/Hub/Efficiencies/EfficiencyData/\"\n",
    "# eff_dataframes_input = eff_retriever(data_path, combine_data)\n",
    "\n",
    "if not combine_data:\n",
    "    try:\n",
    "        remove_index = folders.index(\"Background\")\n",
    "        folders = remover(folders, remove_index)\n",
    "        model_dataframes = remover(model_dataframes, remove_index)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotter import plotter\n",
    "from random import sample\n",
    "\n",
    "data_variables = [\"HT\", \"met\", \"phi_diff\", \"ptmax\"]\n",
    "model_dataframes = [[(dropper(dataframe[0], data_variables), dataframe[1]) for dataframe in dataframes] for dataframes in model_dataframes]\n",
    "\n",
    "file_amounts = [[\"BH_n4_M12.csv\", \"BH_n5_M12.csv\", \"BH_n6_M12.csv\"], [\"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL.csv\"]]\n",
    "model_dataframes_input = sampler(model_dataframes, file_amounts)\n",
    "filenames_input = [[dataframe[1] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "dataframes_input = [[dataframe[0] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "\n",
    "\n",
    "combine = [True, True]\n",
    "xgb_model, xlabels, splits = model(dataframes_input, filenames_input, folders, combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(y_pred, file_amounts):\n",
    "    filenum = np.sum([file_amount if type(file_amount) == int else len(file_amount) for file_amount in file_amounts])\n",
    "    split_num = int(len(y_pred) / filenum)\n",
    "    y_pred = [y_pred[i * split_num : (i + 1) * split_num] for i in range(filenum)]\n",
    "\n",
    "    y_pred_data = [(np.round(y_pred[i].count(0) / split_num, 3), np.round(y_pred[i].count(1) / split_num, 3)) for i in range(len(y_pred))]\n",
    "    y_pred_labels = [((y_pred[i].count(0), np.round(y_pred[i].count(0) / split_num, 3)), (y_pred[i].count(1), np.round(y_pred[i].count(1) / split_num, 3))) for i in range(len(y_pred))]\n",
    "\n",
    "    return y_pred_data, y_pred_labels\n",
    "\n",
    "\n",
    "file_amounts = [18, 3]\n",
    "model_dataframes_input = model_dataframes\n",
    "filenames_input = [[dataframe[1] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "dataframes_input = [[dataframe[0] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "\n",
    "plotter(data_variables, dataframes_input, filenames_input, filter_strengths = [0.99, 0.99, 1, 0.975, 1], binsizes = [50, 50, 0.2, 50, 0.5])\n",
    "\n",
    "x_test, y_test = xy_data(dataframes_input, filenames_input)\n",
    "ylabels = list_unpacker(filenames_input, [])\n",
    "\n",
    "# Make predictions with model\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "y_pred = [split_index for pred_num in y_pred for split_index, split in enumerate(splits) if pred_num in split]\n",
    "\n",
    "\n",
    "\n",
    "y_pred_data, y_pred_labels = split_data(y_pred, file_amounts)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18, 6))\n",
    "plt.style.use(\"seaborn-v0_8-dark\")\n",
    "subplots = fig.subplots(1, 2)\n",
    "\n",
    "\n",
    "x_ticks = np.arange(len(xlabels))   \n",
    "y_ticks = np.arange(len(ylabels))    \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot confusion matrix\n",
    "ax = subplots[0]\n",
    "sns.heatmap(y_pred_data, annot = True, ax = ax, xticklabels = xlabels, yticklabels = ylabels)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), va = \"top\", rotation = 45)\n",
    "\n",
    "# Plot variables by importance for prediction\n",
    "ax = subplots[1]\n",
    "ax.grid()\n",
    "sorted_idx = xgb_model.feature_importances_.argsort()\n",
    "ax.barh(np.array(data_variables)[sorted_idx], xgb_model.feature_importances_[sorted_idx])\n",
    "ax.set_xlabel(\"Xgboost Feature Importance\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sphal_file in range(3):\n",
    "    for black_file in range(18):\n",
    "        model_dataframes_input = [[model_dataframes[0][black_file]], [model_dataframes[1][sphal_file]]]\n",
    "        filenames_input = [[dataframe[1] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "        dataframes_input = [[dataframe[0] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "\n",
    "        print(filenames_input)\n",
    "\n",
    "\n",
    "        combine = [True, True]\n",
    "        xgb_model, xlabels, splits = model(dataframes_input, filenames_input, folders, combine)\n",
    "\n",
    "\n",
    "        file_amounts = [18, 3]\n",
    "        model_dataframes_input = model_dataframes\n",
    "        filenames_input = [[dataframe[1] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "        dataframes_input = [[dataframe[0] for dataframe in dataframes] for dataframes in model_dataframes_input]\n",
    "\n",
    "        # plotter(data_variables, dataframes_input, filenames_input, filter_strengths = [0.99, 0.99, 1, 0.975, 1], binsizes = [50, 50, 0.2, 50, 0.5])\n",
    "\n",
    "        x_test, y_test = xy_data(dataframes_input, filenames_input)\n",
    "        ylabels = list_unpacker(filenames_input, [])\n",
    "\n",
    "        # Make predictions with model\n",
    "        y_pred = xgb_model.predict(x_test)\n",
    "        y_pred = [split_index for pred_num in y_pred for split_index, split in enumerate(splits) if pred_num in split]\n",
    "\n",
    "\n",
    "\n",
    "        y_pred_data, y_pred_labels = split_data(y_pred, file_amounts)\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize = (18, 6))\n",
    "        plt.style.use(\"seaborn-v0_8-dark\")\n",
    "        subplots = fig.subplots(1, 2)\n",
    "\n",
    "\n",
    "        x_ticks = np.arange(len(xlabels))   \n",
    "        y_ticks = np.arange(len(ylabels))    \n",
    "\n",
    "        import seaborn as sns\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        ax = subplots[0]\n",
    "        sns.heatmap(y_pred_data, annot = True, ax = ax, xticklabels = xlabels, yticklabels = ylabels)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), va = \"top\", rotation = 45)\n",
    "\n",
    "        # Plot variables by importance for prediction\n",
    "        ax = subplots[1]\n",
    "        ax.grid()\n",
    "        sorted_idx = xgb_model.feature_importances_.argsort()\n",
    "        ax.barh(np.array(data_variables)[sorted_idx], xgb_model.feature_importances_[sorted_idx])\n",
    "        ax.set_xlabel(\"Xgboost Feature Importance\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "192e37e5372867588bf05c86667329964572fb6f73abd1341e9017e451c6727b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
